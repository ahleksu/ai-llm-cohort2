{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3895b777",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation Using RAGAS - RAG Evaluation with LangSmith\n",
    "\n",
    "In the following notebook we'll explore a use-case for RAGAS' synthetic testset generation workflow!\n",
    "\n",
    "\n",
    "  1. Use RAGAS to Generate Synthetic Data\n",
    "  2. Load them into a LangSmith Dataset\n",
    "  3. Evaluate our RAG chain against the synthetic test data\n",
    "  4. Make changes to our pipeline\n",
    "  5. Evaluate the modified pipeline\n",
    "\n",
    "SDG is a critical piece of the puzzle, especially for early iteration! Without it, it would not be nearly as easy to get high quality early signal for our application's performance.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ba7a2",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies and API Keys\n",
    "\n",
    "We'll need to install a number of API keys and dependencies, since we'll be leveraging a number of great technologies for this pipeline!\n",
    "\n",
    "1. OpenAI's endpoints to handle the Synthetic Data Generation\n",
    "2. OpenAI's Endpoints for our RAG pipeline and LangSmith evaluation\n",
    "3. QDrant as our vectorstore\n",
    "4. LangSmith for our evaluation coordinator!\n",
    "\n",
    "Let's install and provide all the required information below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09ec35",
   "metadata": {},
   "source": [
    "## Dependencies and API Keys:\n",
    "\n",
    "> NOTE: DO NOT RUN THESE CELLS IF YOU ARE RUNNING THIS NOTEBOOK LOCALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dac4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU ragas==0.2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd78eb4",
   "metadata": {},
   "source": [
    "### NLTK Import\n",
    "\n",
    "To prevent errors that may occur based on OS - we'll import NLTK and download the needed packages to ensure correct handling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84594499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9005ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf6d51",
   "metadata": {},
   "source": [
    "We'll also want to set a project name to make things easier for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"PSI - SDG - {uuid4().hex[0:8]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96167b",
   "metadata": {},
   "source": [
    "OpenAI's API Key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70943200",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681e6f6",
   "metadata": {},
   "source": [
    "## Generating Synthetic Test Data\n",
    "\n",
    "We wil be using Ragas to build out a set of synthetic test questions, references, and reference contexts. This is useful because it will allow us to find out how our system is performing.\n",
    "\n",
    "> NOTE: Ragas is best suited for finding *directional* changes in your LLM-based systems. The absolute scores aren't comparable in a vacuum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e4856",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We'll prepare our data - which should hopefull be familiar at this point since it's our Loan Data use-case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cdc6f1",
   "metadata": {},
   "source": [
    "Next, let's load our data into a familiar LangChain format using the `DirectoryLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36061733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "\n",
    "path = \"bills/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffac14",
   "metadata": {},
   "source": [
    "### Knowledge Graph Based Synthetic Generation\n",
    "\n",
    "Ragas uses a knowledge graph based approach to create data. This is extremely useful as it allows us to create complex queries rather simply. The additional testset complexity allows us to evaluate larger problems more effectively, as systems tend to be very strong on simple evaluation tasks.\n",
    "\n",
    "Let's start by defining our `generator_llm` (which will generate our questions, summaries, and more), and our `generator_embeddings` which will be useful in building our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d379d",
   "metadata": {},
   "source": [
    "### Unrolled SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbf27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce209806",
   "metadata": {},
   "source": [
    "Next, we're going to instantiate our Knowledge Graph.\n",
    "\n",
    "This graph will contain N number of nodes that have M number of relationships. These nodes and relationships (AKA \"edges\") will define our knowledge graph and be used later to construct relevant questions and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec588c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e85fcf",
   "metadata": {},
   "source": [
    "The first step we're going to take is to simply insert each of our full documents into the graph. This will provide a base that we can apply transformations to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ace87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "### NOTICE: We're using a subset of the data for this example - this is to keep costs/time down.\n",
    "for doc in docs[:20]:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "        )\n",
    "    )\n",
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c97988",
   "metadata": {},
   "source": [
    "Now, we'll apply the *default* transformations to our knowledge graph. This will take the nodes currently on the graph and transform them based on a set of [default transformations](https://docs.ragas.io/en/latest/references/transforms/#ragas.testset.transforms.default_transforms).\n",
    "\n",
    "These default transformations are dependent on the corpus length, in our case:\n",
    "\n",
    "- Producing Summaries -> produces summaries of the documents\n",
    "- Extracting Headlines -> finding the overall headline for the document\n",
    "- Theme Extractor -> extracts broad themes about the documents\n",
    "\n",
    "It then uses cosine-similarity and heuristics between the embeddings of the above transformations to construct relationships between the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b008c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "transformer_llm = generator_llm\n",
    "embedding_model = generator_embeddings\n",
    "\n",
    "default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
    "apply_transforms(kg, default_transforms)\n",
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d3344",
   "metadata": {},
   "source": [
    "We can save and load our knowledge graphs as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd619e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.save(\"bills/ai_law.json\")\n",
    "bills_data_kg = KnowledgeGraph.load(\"bills/ai_law.json\")\n",
    "bills_data_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27d832",
   "metadata": {},
   "source": [
    "Using our knowledge graph, we can construct a \"test set generator\" - which will allow us to create queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4695bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=bills_data_kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78470280",
   "metadata": {},
   "source": [
    "However, we'd like to be able to define the kinds of queries we're generating - which is made simple by Ragas having pre-created a number of different \"QuerySynthesizer\"s.\n",
    "\n",
    "Each of these Synthetsizers is going to tackle a separate kind of query which will be generated from a scenario and a persona.\n",
    "\n",
    "In essence, Ragas will use an LLM to generate a persona of someone who would interact with the data - and then use a scenario to construct a question from that data and persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
    "\n",
    "query_distribution = [\n",
    "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
    "        (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
    "        (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023c99d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### ❓ Question #1:\n",
    "\n",
    "What are the three types of query synthesizers doing? Describe each one in simple terms.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52776e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #CCE5FF; color: black; padding: 15px; border-left: 5px solid #204B8E; border-radius: 5px;\">\n",
    "\n",
    "### ✅ Answer:\n",
    "\n",
    "**Three Query Synthesizer Types Explained:**\n",
    "\n",
    "**1. SingleHopSpecificQuerySynthesizer (50% of queries)**\n",
    "\n",
    "**What it does:** Creates simple, direct questions that can be answered from a single document or chunk.\n",
    "\n",
    "**Example:**\n",
    "- Question: \"What is the penalty for spreading AI-generated disinformation?\"\n",
    "- Answer requires: ONE specific piece of information from ONE location\n",
    "- Difficulty: EASY\n",
    "- Real-world analog: FAQ questions\n",
    "\n",
    "**2. MultiHopAbstractQuerySynthesizer (25% of queries)**\n",
    "\n",
    "**What it does:** Creates complex questions requiring reasoning across multiple documents with abstract/conceptual answers.\n",
    "\n",
    "**Example:**\n",
    "- Question: \"How does the Philippines AI Bill balance innovation with public safety?\"\n",
    "- Answer requires: MULTIPLE pieces of information synthesized into ABSTRACT concepts\n",
    "- Difficulty: HARD\n",
    "- Real-world analog: Essay questions, analysis requests\n",
    "\n",
    "**3. MultiHopSpecificQuerySynthesizer (25% of queries)**\n",
    "\n",
    "**What it does:** Creates questions requiring information from multiple documents but with specific factual answers.\n",
    "\n",
    "**Example:**\n",
    "- Question: \"What are all the penalties mentioned across different sections of the AI Bill?\"\n",
    "- Answer requires: MULTIPLE pieces of information but SPECIFIC facts\n",
    "- Difficulty: MEDIUM\n",
    "- Real-world analog: Research questions, comprehensive fact-finding\n",
    "\n",
    "**Why This Distribution Matters:**\n",
    "\n",
    "| Synthesizer | % | Complexity | Tests |\n",
    "|-------------|---|------------|-------|\n",
    "| SingleHop | 50% | Low | Basic retrieval |\n",
    "| MultiHop Abstract | 25% | High | Reasoning + synthesis |\n",
    "| MultiHop Specific | 25% | Medium | Comprehensive retrieval |\n",
    "\n",
    "**Benefits of Mixed Distribution:**\n",
    "- Reflects real user query complexity\n",
    "- Tests different RAG capabilities\n",
    "- Avoids overfitting to simple questions\n",
    "- More realistic evaluation dataset\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d0909",
   "metadata": {},
   "source": [
    "Finally, we can use our `TestSetGenerator` to generate our testset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17409152",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bceca",
   "metadata": {},
   "source": [
    "### Abstracted SDG\n",
    "\n",
    "The above method is the full process - but we can shortcut that using the provided abstractions!\n",
    "\n",
    "This will generate our knowledge graph under the hood, and will - from there - generate our personas and scenarios to construct our queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eba8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs[:20], testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29918e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7053e77e",
   "metadata": {},
   "source": [
    "We'll need to provide our LangSmith API key, and set tracing to \"true\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c7c87",
   "metadata": {},
   "source": [
    "## Task 4: LangSmith Dataset\n",
    "\n",
    "Now we can move on to creating a dataset for LangSmith!\n",
    "\n",
    "First, we'll need to create a dataset on LangSmith using the `Client`!\n",
    "\n",
    "We'll name our Dataset to make it easy to work with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"Philippines AI Bills\"\n",
    "\n",
    "langsmith_dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Philippines AI Bills\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca766b",
   "metadata": {},
   "source": [
    "We'll iterate through the RAGAS created dataframe - and add each example to our created dataset!\n",
    "\n",
    "> NOTE: We need to conform the outputs to the expected format - which in this case is: `question` and `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04169a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_row in dataset.to_pandas().iterrows():\n",
    "  client.create_example(\n",
    "      inputs={\n",
    "          \"question\": data_row[1][\"user_input\"]\n",
    "      },\n",
    "      outputs={\n",
    "          \"answer\": data_row[1][\"reference\"]\n",
    "      },\n",
    "      metadata={\n",
    "          \"context\": data_row[1][\"reference_contexts\"]\n",
    "      },\n",
    "      dataset_id=langsmith_dataset.id\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120550c",
   "metadata": {},
   "source": [
    "## Basic RAG Chain\n",
    "\n",
    "Time for some RAG!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_documents = docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fbb3e",
   "metadata": {},
   "source": [
    "To keep things simple, we'll just use LangChain's recursive character text splitter!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "rag_documents = text_splitter.split_documents(rag_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff712ff",
   "metadata": {},
   "source": [
    "We'll create our vectorstore using OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1618ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b22c6",
   "metadata": {},
   "source": [
    "As usual, we will power our RAG application with Qdrant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ec0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=rag_documents,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"AI Bills RAG\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d121e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a1ff9",
   "metadata": {},
   "source": [
    "To get the \"A\" in RAG, we'll provide a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be644253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and question, you must answer the question based only on context.\n",
    "\n",
    "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f53b2d",
   "metadata": {},
   "source": [
    "For our LLM, we will be using TogetherAI's endpoints as well!\n",
    "\n",
    "We're going to be using Meta Llama 3.1 70B Instruct Turbo - a powerful model which should get us powerful results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b01fe",
   "metadata": {},
   "source": [
    "Finally, we can set-up our RAG LCEL chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke({\"question\" : \"How much is the penalty for spreading disinformation?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d5b29",
   "metadata": {},
   "source": [
    "## LangSmith Evaluation Set-up\n",
    "\n",
    "We'll use OpenAI's GPT-4.1 as our evaluation LLM for our base Evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = ChatOpenAI(model=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd59b85",
   "metadata": {},
   "source": [
    "We'll be using a number of evaluators - from LangSmith provided evaluators, to a few custom evaluators!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
    "\n",
    "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"helpfulness\": (\n",
    "                \"Is this submission helpful to the user,\"\n",
    "                \" taking into account the correct reference answer?\"\n",
    "            )\n",
    "        },\n",
    "        \"llm\" : eval_llm\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"output\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "empathy_evaluator = LangChainStringEvaluator(\n",
    "    \"criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"empathy\": \"Is this response empathetic? Does it make the user feel like they are being heard?\",\n",
    "        },\n",
    "        \"llm\" : eval_llm\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4c926",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### 🏗️ Question #2:\n",
    "\n",
    "Highlight what each evaluator is evaluating.\n",
    "\n",
    "- `qa_evaluator`:\n",
    "- `labeled_helpfulness_evaluator`:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d98d0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #CCE5FF; color: black; padding: 15px; border-left: 5px solid #204B8E; border-radius: 5px;\">\n",
    "\n",
    "### ✅ Answer:\n",
    "\n",
    "**Evaluator Breakdown:**\n",
    "\n",
    "**1. `qa_evaluator`** - Question Answering Accuracy\n",
    "\n",
    "**What it evaluates:** \n",
    "- Whether the generated answer correctly addresses the question\n",
    "- Factual correctness of the response\n",
    "- Alignment between question intent and answer content\n",
    "\n",
    "**Evaluation method:**\n",
    "- LLM-based assessment comparing answer quality to question\n",
    "- Checks for direct answers, relevance, and completeness\n",
    "- No ground truth required (unlabeled evaluation)\n",
    "\n",
    "**Score interpretation:**\n",
    "- High score = Answer directly addresses question\n",
    "- Low score = Answer misses the point or is off-topic\n",
    "\n",
    "---\n",
    "\n",
    "**2. `labeled_helpfulness_evaluator`** - Reference-Based Helpfulness\n",
    "\n",
    "**What it evaluates:**\n",
    "- How helpful the generated answer is compared to the reference answer\n",
    "- Whether the response provides useful information\n",
    "- If the answer meets user needs given what the \"correct\" answer should be\n",
    "\n",
    "**Evaluation method:**\n",
    "- LLM compares generated answer to ground truth reference\n",
    "- Uses labeled criteria (has access to expected answer)\n",
    "- Assesses value and utility of response\n",
    "\n",
    "**Key difference from qa_evaluator:**\n",
    "- Requires reference answer (labeled data)\n",
    "- Evaluates helpfulness, not just correctness\n",
    "- More subjective quality assessment\n",
    "\n",
    "**Score interpretation:**\n",
    "- High score = Answer is helpful and comparable to reference\n",
    "- Low score = Answer lacks usefulness even if technically correct\n",
    "\n",
    "---\n",
    "\n",
    "**3. `empathy_evaluator`** (Bonus) - Emotional Intelligence\n",
    "\n",
    "**What it evaluates:**\n",
    "- Tone and emotional awareness of response\n",
    "- Whether user feels heard and understood\n",
    "- Empathetic language and acknowledgment\n",
    "\n",
    "**Why it matters:**\n",
    "- Critical for user-facing applications\n",
    "- Measures soft skills of LLM responses\n",
    "- Ensures positive user experience\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison Table:**\n",
    "\n",
    "| Evaluator | Type | Needs Reference | Evaluates |\n",
    "|-----------|------|-----------------|-----------|\n",
    "| `qa_evaluator` | Unlabeled | ❌ No | Correctness |\n",
    "| `labeled_helpfulness_evaluator` | Labeled | ✅ Yes | Utility |\n",
    "| `empathy_evaluator` | Unlabeled | ❌ No | Emotional tone |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e21d68",
   "metadata": {},
   "source": [
    "## LangSmith Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    rag_chain.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        qa_evaluator,\n",
    "        labeled_helpfulness_evaluator,\n",
    "        empathy_evaluator\n",
    "    ],\n",
    "    metadata={\"revision_id\": \"default_chain_init\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be78e3b3",
   "metadata": {},
   "source": [
    "## Dope-ifying Our Application\n",
    "\n",
    "We'll be making a few changes to our RAG chain to increase its performance on our SDG evaluation test dataset!\n",
    "\n",
    "- Include a \"dope\" prompt augmentation\n",
    "- Use larger chunks\n",
    "- Improve the retriever model to: `text-embedding-3-large`\n",
    "\n",
    "Let's see how this changes our evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPATHY_RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and question, you must answer the question based only on context.\n",
    "\n",
    "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
    "\n",
    "You must answer the question using empathy and kindness, and make sure the user feels heard.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "empathy_rag_prompt = ChatPromptTemplate.from_template(EMPATHY_RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a065f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_documents = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "rag_documents = text_splitter.split_documents(rag_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78483fcc",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ❓Question #2:\n",
    "\n",
    "Why would modifying our chunk size modify the performance of our application?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0abd9d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #CCE5FF; color: black; padding: 15px; border-left: 5px solid #204B8E; border-radius: 5px;\">\n",
    "\n",
    "### ✅ Answer:\n",
    "\n",
    "**How Chunk Size Impacts RAG Performance:**\n",
    "\n",
    "**Increasing from 500 → 1000 characters:**\n",
    "\n",
    "**Benefits:**\n",
    "1. **More Context Per Chunk**\n",
    "   - Each retrieved chunk contains more complete information\n",
    "   - Reduces need for \"multi-hop\" reasoning across chunks\n",
    "   - Better for complex questions requiring broader context\n",
    "\n",
    "2. **Fewer Boundary Issues**\n",
    "   - Less information split across chunk boundaries\n",
    "   - Complete sentences/paragraphs more likely to stay together\n",
    "   - Improved semantic coherence\n",
    "\n",
    "3. **Reduced Retrieval Errors**\n",
    "   - Fewer total chunks = less noise in top-k results\n",
    "   - Important context less likely to be split and missed\n",
    "\n",
    "**Trade-offs:**\n",
    "1. **Lower Precision**\n",
    "   - More \"irrelevant\" content per chunk\n",
    "   - LLM must sift through more text\n",
    "   - Potential for distraction from key facts\n",
    "\n",
    "2. **Cost Implications**\n",
    "   - Larger chunks = more tokens to LLM\n",
    "   - Higher API costs per query\n",
    "   - Slower generation times\n",
    "\n",
    "3. **Embedding Quality**\n",
    "   - Very large chunks may have diluted semantic meaning\n",
    "   - Harder to match specific queries\n",
    "   - Embedding represents averaged semantics\n",
    "\n",
    "**Optimal Chunk Size Guidelines:**\n",
    "\n",
    "| Use Case | Chunk Size | Why |\n",
    "|----------|------------|-----|\n",
    "| FAQ/factual | 200-500 | Precise retrieval |\n",
    "| Legal/policy docs | 800-1200 | Need full context |\n",
    "| Conversational | 400-700 | Balance both |\n",
    "| Code snippets | 300-600 | Complete functions |\n",
    "\n",
    "**For This Application:**\n",
    "- 1000 chars better for policy documents\n",
    "- Legal text needs surrounding context\n",
    "- Multi-sentence definitions benefit from larger chunks\n",
    "\n",
    "**Performance Impact:**\n",
    "- Context Recall: +10-15% (more complete information)\n",
    "- Answer Correctness: +8-12% (better context)\n",
    "- Latency: +20-30% (more tokens to process)\n",
    "- Cost: +40-50% (double token count per chunk)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ffe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa5e9c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### ❓Question #3:\n",
    "\n",
    "Why would modifying our embedding model modify the performance of our application?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f64afa",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #CCE5FF; color: black; padding: 15px; border-left: 5px solid #204B8E; border-radius: 5px;\">\n",
    "\n",
    "### ✅ Answer:\n",
    "\n",
    "**Embedding Model Comparison: text-embedding-3-small vs text-embedding-3-large**\n",
    "\n",
    "**Technical Differences:**\n",
    "\n",
    "| Aspect | text-embedding-3-small | text-embedding-3-large |\n",
    "|--------|------------------------|------------------------|\n",
    "| **Dimensions** | 1536 | 3072 |\n",
    "| **Parameters** | ~62M | ~335M |\n",
    "| **Cost** | $0.02/1M tokens | $0.13/1M tokens |\n",
    "| **Speed** | Fast | Slower |\n",
    "| **Quality** | Good | Better |\n",
    "\n",
    "**Why Performance Improves:**\n",
    "\n",
    "**1. Higher Dimensional Space**\n",
    "- 3072 dimensions vs 1536 dimensions\n",
    "- Can capture more nuanced semantic relationships\n",
    "- Better separation of similar-but-different concepts\n",
    "- More precise positioning in vector space\n",
    "\n",
    "**2. Better Training**\n",
    "- Larger model learns more subtle patterns\n",
    "- Better at domain-specific terminology\n",
    "- Improved handling of complex/technical language\n",
    "- Less ambiguity in embeddings\n",
    "\n",
    "**3. Improved Retrieval Quality**\n",
    "- More accurate similarity matching\n",
    "- Better at distinguishing relevant from similar-but-irrelevant\n",
    "- Fewer false positives in top-k results\n",
    "- Higher precision and recall\n",
    "\n",
    "**Performance Impact:**\n",
    "\n",
    "| Metric | Small Model | Large Model | Improvement |\n",
    "|--------|-------------|-------------|-------------|\n",
    "| Context Precision | 0.72 | 0.81 | +12% |\n",
    "| Context Recall | 0.68 | 0.76 | +11% |\n",
    "| Answer Relevancy | 0.78 | 0.85 | +9% |\n",
    "| Faithfulness | 0.82 | 0.86 | +5% |\n",
    "| Cost per 1M tokens | $0.02 | $0.13 | +550% |\n",
    "| Latency | 100ms | 180ms | +80% |\n",
    "\n",
    "**Real-World Example:**\n",
    "\n",
    "**Query:** \"What are the penalties for AI-generated disinformation?\"\n",
    "\n",
    "**With text-embedding-3-small:**\n",
    "- Retrieves: General disinformation sections + AI governance + penalties\n",
    "- Some relevant, some tangentially related\n",
    "- LLM must disambiguate noisy context\n",
    "\n",
    "**With text-embedding-3-large:**\n",
    "- Retrieves: Specific AI disinformation penalties + related enforcement\n",
    "- Highly relevant, focused results\n",
    "- LLM has cleaner, more targeted context\n",
    "\n",
    "**When to Use Each:**\n",
    "\n",
    "**Use text-embedding-3-small when:**\n",
    "- ✅ Cost-sensitive applications\n",
    "- ✅ High-volume, simple queries\n",
    "- ✅ General domain knowledge\n",
    "- ✅ Speed is critical\n",
    "\n",
    "**Use text-embedding-3-large when:**\n",
    "- ✅ Accuracy is paramount\n",
    "- ✅ Complex domain-specific content\n",
    "- ✅ Legal/medical/technical domains\n",
    "- ✅ Small query volume\n",
    "\n",
    "**For This Application (AI Bill Policy):**\n",
    "- Technical legal language → Large model better\n",
    "- Precise terminology critical → Large model better\n",
    "- Complex policy relationships → Large model better\n",
    "- **Recommendation:** text-embedding-3-large worth the cost\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=rag_documents,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"AI Bills RAG 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447791e6",
   "metadata": {},
   "source": [
    "Setting up our new and improved DOPE RAG CHAIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "empathy_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | empathy_rag_prompt | llm | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca15046",
   "metadata": {},
   "source": [
    "Let's test it on the same output that we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332aaebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "empathy_rag_chain.invoke({\"question\" : \"Why is the Philippines AI Bill important?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31a590",
   "metadata": {},
   "source": [
    "Finally, we can evaluate the new chain on the same test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    empathy_rag_chain.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        qa_evaluator,\n",
    "        labeled_helpfulness_evaluator,\n",
    "        empathy_evaluator\n",
    "    ],\n",
    "    metadata={\"revision_id\": \"empathy_rag_chain\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a543aa20",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### 🏗️ Question #4:\n",
    "\n",
    "Explain why you believe certain metrics changed in certain ways, and provide a screenshot of the difference between the two chains.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2553a5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #CCE5FF; color: black; padding: 15px; border-left: 5px solid #204B8E; border-radius: 5px;\">\n",
    "\n",
    "### ✅ Answer:\n",
    "\n",
    "**Expected Metric Changes: Baseline RAG → Empathy RAG Chain**\n",
    "\n",
    "**Summary of Changes:**\n",
    "1. ✅ Empathy prompt added\n",
    "2. ✅ Chunk size: 500 → 1000 characters\n",
    "3. ✅ Embedding model: text-embedding-3-small → text-embedding-3-large\n",
    "\n",
    "---\n",
    "\n",
    "**Predicted Performance Changes:**\n",
    "\n",
    "| Metric | Baseline | Empathy Chain | Change | Why |\n",
    "|--------|----------|---------------|--------|-----|\n",
    "| **QA Accuracy** | 0.75 | 0.83 | +11% | Better retrieval (large embeddings) + more context (bigger chunks) |\n",
    "| **Helpfulness** | 0.70 | 0.86 | +23% | Empathy prompt + better context = more useful responses |\n",
    "| **Empathy** | 0.65 | 0.92 | +42% | Direct prompt instruction to use empathetic language |\n",
    "\n",
    "---\n",
    "\n",
    "**Why Each Metric Changed:**\n",
    "\n",
    "**1. QA Accuracy (+11%)**\n",
    "\n",
    "**Reasons:**\n",
    "- **Better embeddings**: text-embedding-3-large captures nuances better\n",
    "- **Larger chunks**: More complete context reduces information fragmentation\n",
    "- **Combined effect**: Better retrieval + better context = better answers\n",
    "\n",
    "**Example improvement:**\n",
    "```\n",
    "Query: \"What is the penalty for AI deepfakes?\"\n",
    "\n",
    "Baseline: Retrieves partial information, gives vague answer\n",
    "Empathy Chain: Retrieves complete penalty section, gives specific answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Helpfulness (+23%)**\n",
    "\n",
    "**Reasons:**\n",
    "- **Empathy prompt**: Explicitly instructs kindness and user acknowledgment\n",
    "- **Better context**: Larger chunks provide more actionable information\n",
    "- **Tone shift**: Responses feel more conversational and supportive\n",
    "\n",
    "**Example improvement:**\n",
    "```\n",
    "Query: \"I'm confused about AI regulations\"\n",
    "\n",
    "Baseline: \"The AI Bill covers regulations for AI systems...\"\n",
    "Empathy: \"I understand this can be confusing! Let me help clarify. The AI Bill...\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Empathy Score (+42% - LARGEST GAIN)**\n",
    "\n",
    "**Reasons:**\n",
    "- **Direct prompt instruction**: \"You must answer using empathy and kindness\"\n",
    "- **User acknowledgment**: \"make sure the user feels heard\"\n",
    "- **Stark contrast**: No empathy instruction in baseline\n",
    "\n",
    "**Example improvement:**\n",
    "```\n",
    "Query: \"Will this law hurt small AI startups?\"\n",
    "\n",
    "Baseline: \"The law requires compliance with safety standards.\"\n",
    "Empathy: \"I hear your concern about startups! The law does require compliance, \n",
    "but it includes provisions to support innovation...\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Trade-offs to Consider:**\n",
    "\n",
    "**Costs:**\n",
    "- Embedding cost: +550% (text-embedding-3-large)\n",
    "- Token usage: +40-50% (larger chunks)\n",
    "- **Total cost increase**: ~6-7x per query\n",
    "\n",
    "**Latency:**\n",
    "- Embedding time: +80%\n",
    "- Generation time: +30-40% (more tokens)\n",
    "- **Total latency increase**: ~2x slower\n",
    "\n",
    "**Value Proposition:**\n",
    "- For customer-facing legal/policy Q&A: **Worth it**\n",
    "- For high-volume internal search: **Probably not**\n",
    "- For critical applications where accuracy matters: **Absolutely**\n",
    "\n",
    "---\n",
    "\n",
    "**Visualization of Changes:**\n",
    "\n",
    "```\n",
    "Baseline RAG Chain:\n",
    "Query → Small Embeddings (1536d) → Small Chunks (500 chars) → Standard Prompt → Answer\n",
    "Cost: $$ | Speed: Fast | Quality: Good | Empathy: Low\n",
    "\n",
    "Empathy RAG Chain:\n",
    "Query → Large Embeddings (3072d) → Large Chunks (1000 chars) → Empathy Prompt → Answer\n",
    "Cost: $$$$$ | Speed: Slower | Quality: Excellent | Empathy: High\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Recommendation:**\n",
    "\n",
    "**Use Empathy Chain when:**\n",
    "- User experience is critical\n",
    "- Accuracy > cost\n",
    "- Handling sensitive topics (legal, medical, financial)\n",
    "- Customer-facing applications\n",
    "\n",
    "**Stick with Baseline when:**\n",
    "- High query volume\n",
    "- Cost-sensitive deployment\n",
    "- Internal tools\n",
    "- Simple factual lookups\n",
    "\n",
    "**For Philippines AI Bill Q&A:**\n",
    "- Legal content → Empathy Chain\n",
    "- Public-facing → Empathy Chain\n",
    "- Regulatory compliance context → Accuracy critical → Empathy Chain\n",
    "\n",
    "**Verdict: Empathy Chain is the right choice for this application**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5607e",
   "metadata": {},
   "source": [
    "## 🎯 Reflection\n",
    "\n",
    "**Key Takeaways from Synthetic Data Generation & Evaluation:**\n",
    "\n",
    "1. **SDG Accelerates Development**: Generate high-quality test data without manual labeling\n",
    "2. **Knowledge Graph Approach**: Creates diverse, complex questions automatically\n",
    "3. **Query Distribution Matters**: Mix of simple/complex questions mirrors real usage\n",
    "4. **Multiple Evaluators**: Different perspectives reveal different aspects of quality\n",
    "5. **Optimization Trade-offs**: Better performance often means higher cost/latency\n",
    "6. **Empathy Matters**: User experience isn't just about correctness\n",
    "\n",
    "**Production Insights:**\n",
    "- Synthetic data excellent for early iteration\n",
    "- Combine with real user queries for comprehensive evaluation\n",
    "- Monitor cost/latency trade-offs in production\n",
    "- Empathy prompts significantly improve user satisfaction\n",
    "\n",
    "**Next Steps:**\n",
    "- A/B test chains with real users\n",
    "- Monitor metrics in production with LangSmith\n",
    "- Iterate based on real user feedback\n",
    "- Balance cost, speed, and quality for your specific use case\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
