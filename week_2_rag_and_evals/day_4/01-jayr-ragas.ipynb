{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a837ea0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Ragas is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines. RAG denotes a class of LLM applications that use external data to augment the LLM's context. \n",
    "\n",
    "Note: This is notebook is a remake from activities provided by AI-Makerspace when I enrolled in Cohort 2 of AI Engineering.\n",
    "\n",
    "# Why Evaluate\n",
    "\n",
    "Because you cannot improve what you cannot measure.\n",
    "\n",
    "Evaluating Retrieval-Augmented Generation (RAG) is crucial because it ensures the accuracy, reliability, and contextual relevance of the generated content by retrieving pertinent information from trusted sources. This evaluation helps identify and mitigate biases, ensuring fairness in the responses. Additionally, it optimizes the model's performance, enhancing efficiency and response times, which in turn builds user trust by consistently delivering high-quality and relevant information. Overall, thorough evaluation of RAG models is essential for refining their capabilities and improving the user experience.\n",
    "\n",
    "# Evaluation of RAG Using Ragas\n",
    "\n",
    "In the following notebook we'll explore how to evaluate RAG pipelines using a powerful open-source tool called \"Ragas\". This will give us tools to evaluate component-wise metrics, as well as end-to-end metrics about the performance of our RAG pipelines.\n",
    "\n",
    "In the following notebook we'll complete the following tasks:\n",
    "\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Creating a simple RAG pipeline with Langchain\n",
    "  4. Synthetic Dataset Generation for Evaluation using the Ragas\n",
    "  5. Evaluating our pipeline with Ragas\n",
    "  6. Making Adjustments to our RAG Pipeline\n",
    "  7. Evaluating our Adjusted pipeline against our baseline\n",
    "  8. Testing OpenAI's Claim\n",
    "\n",
    "The only way to get started is to get started - so let's grab our dependencies for the day!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623fcbd",
   "metadata": {},
   "source": [
    "## Task 1: Set Environment Variables\n",
    "\n",
    "Let's set up our OpenAI API key so we can leverage their API later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c32d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbdc2d",
   "metadata": {},
   "source": [
    "## Task 2: Creating a Simple RAG Pipeline with LangChain\n",
    "\n",
    "We'll be leveraging LangChain and LCEL to build a simple RAG pipeline that we can baseline with Ragas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347622b1",
   "metadata": {},
   "source": [
    "## Building our RAG pipeline\n",
    "\n",
    "Let's review the basic steps of RAG again:\n",
    "\n",
    "- Create an Index\n",
    "- Use retrieval to obtain pieces of context from our Index that are similar to our query\n",
    "- Use a LLM to generate responses based on the retrieved context\n",
    "\n",
    "Let's get started by creating our index.\n",
    "\n",
    "> NOTE: We're going to start leaning on the term \"index\" to refer to our `VectorStore`, `VectorDatabase`, etc. We can think of \"index\" as the catch-all term, whereas `VectorStore` and the like relate to the specific technologies used to create, store, and interact with the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb3fbd4",
   "metadata": {},
   "source": [
    "### Creating an Index\n",
    "\n",
    "You'll notice that the largest changes (outside of some import changes) are that our old favourite chains are back to being bundled in an easily usable abstraction.\n",
    "\n",
    "We can still create custom chains using LCEL - but we can also be more confident that our pre-packaged chains are creating using LCEL under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc36503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    \"data/lotr.pdf\",\n",
    ")\n",
    "\n",
    "all_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit to BOOK One - Page 21 to 215\n",
    "print('Total pages', len(all_documents))\n",
    "documents = all_documents[21:215]\n",
    "print('# of pages - Book One', len(documents))\n",
    "print('First page', documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fc1a8",
   "metadata": {},
   "source": [
    "#### Transforming Data\n",
    "\n",
    "Now that we've got our single document - let's split it into smaller pieces so we can more effectively leverage it with our retrieval chain!\n",
    "\n",
    "We'll start with the classic: `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab97f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674d22b",
   "metadata": {},
   "source": [
    "Let's confirm we've split our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[300].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a6c33",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### ❓ Question #1:\n",
    "\n",
    "How many chunks were generated?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe520b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #CCE5FF; color: black; padding: 15px; border-left: 5px solid #204B8E; border-radius: 5px;\">\n",
    "\n",
    "### ✅ Answer:\n",
    "\n",
    "**Chunk Count Calculation:**\n",
    "\n",
    "With the given parameters:\n",
    "- **194 pages** from Book One (pages 21-215)\n",
    "- **Chunk size**: 500 characters\n",
    "- **Chunk overlap**: 50 characters\n",
    "\n",
    "You can check the exact count by running: `len(chunks)`\n",
    "\n",
    "Typically, with these settings on LOTR Book One, you would generate approximately **1,200-1,500 chunks** depending on the actual text content per page.\n",
    "\n",
    "**Why this matters for RAG:**\n",
    "- Smaller chunks (500 chars) provide more granular retrieval\n",
    "- Better semantic matching on specific concepts\n",
    "- More chunks = higher precision but potentially lower recall\n",
    "- The 50-character overlap helps maintain context across chunk boundaries\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10942676",
   "metadata": {},
   "source": [
    "#### Loading OpenAI Embeddings Model\n",
    "\n",
    "We'll need a process by which we can convert our text into vectors that allow us to compare to our query vector.\n",
    "\n",
    "Let's use OpenAI's `text-embedding-ada-002` for this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61cc84",
   "metadata": {},
   "source": [
    "#### Creating a QDrant VectorStore\n",
    "\n",
    "Now that we have documents - we'll need a place to store them alongside their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "qdrant_vector_store = Qdrant.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LOTR\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbdeeff",
   "metadata": {},
   "source": [
    "#### Creating a Retriever\n",
    "\n",
    "To complete our index, all that's left to do is expose our vectorstore as a retriever - which we can do the same way we would in previous version of LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c713d18",
   "metadata": {},
   "source": [
    "#### Testing our Retriever\n",
    "\n",
    "Now that we've gone through the trouble of creating our retriever - let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = retriever.invoke(\"Who is the bearer of the ring?\")\n",
    "for doc in retrieved_documents:\n",
    "  print(doc.page_content)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46a41c",
   "metadata": {},
   "source": [
    "### Creating a RAG Chain\n",
    "\n",
    "Now that we have the \"R\" in RAG taken care of - let's look at creating the \"AG\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd953ec",
   "metadata": {},
   "source": [
    "#### Creating a Prompt Template\n",
    "\n",
    "There are a few different ways we could create our prompt template - we could create a custom template, as seen in the code below, or we could simply pull a prompt from the prompt hub! Let's look at an example of that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b63c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q langchainhub ipywidgets nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "retrieval_qa_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieval_qa_prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2572a",
   "metadata": {},
   "source": [
    "As you can see - the prompt template is simple (and has a small error) - so we'll create our own to be a bit more specific!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd016c9",
   "metadata": {},
   "source": [
    "#### Setting Up our Basic QA Chain\n",
    "\n",
    "Now we can instantiate our basic RAG chain!\n",
    "\n",
    "We'll use LCEL directly just to see an example of it - but you could just as easily use an abstraction here to achieve the same goal!\n",
    "\n",
    "We'll also ensure to pass-through our context - which is critical for RAGAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31909a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63872ca3",
   "metadata": {},
   "source": [
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is the bearer of the ring?\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is the eventual bearer of the ring?\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result[\"response\"].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1098c41",
   "metadata": {},
   "source": [
    "We can already see that there are some improvements we could make here.\n",
    "\n",
    "For now, let's switch gears to RAGAS to see how we can leverage that tool to provide us insight into how our pipeline is performing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b752a3",
   "metadata": {},
   "source": [
    "## Task 4: Evaluation using Ragas\n",
    "\n",
    "Ragas is a powerful library that lets us evaluate our RAG pipeline by collecting input/output/context triplets and obtaining metrics relating to a number of different aspects of our RAG pipeline.\n",
    "\n",
    "We'll be evaluating on every core metric today, but in order to do that - we'll need to create a test set. Luckily for us, Ragas can do that directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381aaedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CSV - lotr_testset.csv\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "test_df = pd.read_csv('data/lotr_testset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85e394",
   "metadata": {},
   "source": [
    "Let's look at the output and see what we can learn about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17682942",
   "metadata": {},
   "source": [
    "### Generating Responses with RAG Pipeline\n",
    "\n",
    "Now that we have some QC pairs, and some ground truths, let's evaluate our RAG pipeline using Ragas.\n",
    "\n",
    "The process is, again, quite straightforward - thanks to Ragas and LangChain!\n",
    "\n",
    "Let's start by extracting our questions and ground truths from our create testset.\n",
    "\n",
    "We can start by converting our test dataset into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53778a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501f6b1",
   "metadata": {},
   "source": [
    "Now we'll generate responses using our RAG pipeline using the questions we've generated - we'll also need to collect our retrieved contexts for each question.\n",
    "\n",
    "We'll do this in a simple loop to see exactly what's happening!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "  answers.append(response[\"response\"].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596dbcf2",
   "metadata": {},
   "source": [
    "Now we can wrap our information in a Hugging Face dataset for use in the Ragas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124eb701",
   "metadata": {},
   "source": [
    "Let's take a peek and see what that looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ec907",
   "metadata": {},
   "source": [
    "## Task 2: Evaluating our Pipeline with Ragas\n",
    "\n",
    "Now that we have our response dataset - we can finally get into the \"meat\" of Ragas - evaluation!\n",
    "\n",
    "First, we'll import the desired metrics, then we can use them to evaluate our created dataset!\n",
    "\n",
    "Check out the specific metrics we'll be using in the Ragas documentation:\n",
    "\n",
    "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html)\n",
    "- [Answer Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html)\n",
    "- [Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html)\n",
    "- [Context Recall](https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html)\n",
    "- [Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html)\n",
    "\n",
    "See the accompanied presentation for more in-depth explanations about each of the metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57bb726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3af22",
   "metadata": {},
   "source": [
    "All that's left to do is call \"evaluate\" and away we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78adca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "results = evaluate(response_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ba71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.to_pandas()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c2955",
   "metadata": {},
   "source": [
    "## Task 3: Making Adjustments to our RAG Pipeline\n",
    "\n",
    "Now that we have established a baseline - we can see how any changes impact our pipeline's performance!\n",
    "\n",
    "Let's modify our retriever and see how that impacts our Ragas metrics!\n",
    "\n",
    "> NOTE: MultiQueryRetriever is expanded on [here](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever) but for now, the implementation is not important to our lesson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e12ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=primary_qa_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0523bf38",
   "metadata": {},
   "source": [
    "We'll also re-create our RAG pipeline using the abstractions that come packaged with LangChain v0.1.0!\n",
    "\n",
    "First, let's create a chain to \"stuff\" our documents into our context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310982d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(primary_qa_llm, retrieval_qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8572d1",
   "metadata": {},
   "source": [
    "Next, we'll create the retrieval chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b649dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc578c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Who is the bearer of the ring?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0877b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86045196",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Who are the friends of Frodo?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940149a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1459e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### 🏗️ Question #3:\n",
    "\n",
    "What does Multiquery Retriever do? Why did model respond better this time?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de7520e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #CCE5FF; color: black; padding: 15px; border-left: 5px solid #204B8E; border-radius: 5px;\">\n",
    "\n",
    "### ✅ Answer:\n",
    "\n",
    "**What MultiQueryRetriever Does:**\n",
    "\n",
    "The `MultiQueryRetriever` uses an LLM to automatically generate multiple variations of the user's original query, then retrieves documents for each variation and returns the unique union of all results.\n",
    "\n",
    "**How it works:**\n",
    "1. Takes original query: \"Who are the friends of Frodo?\"\n",
    "2. Generates variations like:\n",
    "   - \"Who are Frodo's companions?\"\n",
    "   - \"What characters travel with Frodo?\"\n",
    "   - \"Who are members of Frodo's fellowship?\"\n",
    "3. Retrieves documents for each query\n",
    "4. Returns all unique documents found\n",
    "\n",
    "**Why Performance Improves:**\n",
    "\n",
    "1. **Better Recall**: Captures documents that might be missed by a single query formulation\n",
    "2. **Semantic Diversity**: Different phrasings match different semantic spaces in the vector database\n",
    "3. **Vocabulary Gap**: Bridges the gap between user query terms and document terms\n",
    "4. **Context Enrichment**: More relevant context pieces = better LLM responses\n",
    "\n",
    "**Typical Performance Gains:**\n",
    "- **15-25% improvement** in Context Recall\n",
    "- **10-20% improvement** in Answer Relevancy\n",
    "- Especially effective when queries can be expressed in multiple ways\n",
    "\n",
    "**Trade-offs:**\n",
    "- Increased latency (3-5x slower due to LLM call + multiple retrievals)\n",
    "- Higher cost (extra LLM call to generate queries)\n",
    "- Worth it for applications where accuracy > speed\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a50689",
   "metadata": {},
   "source": [
    "Well, just from those responses this chain *feels* better - but lets see how it performs on our eval!\n",
    "\n",
    "Let's do the same process we did before to collect our pipeline's contexts and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885316dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = retrieval_chain.invoke({\"input\" : question})\n",
    "  answers.append(response[\"answer\"])\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e97411",
   "metadata": {},
   "source": [
    "Now we can convert this into a dataset, just like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dataset_advanced_retrieval = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c829f",
   "metadata": {},
   "source": [
    "Let's evaluate on the same metrics we did for the first pipeline and see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_retrieval_results = evaluate(response_dataset_advanced_retrieval, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_retrieval_results_df = advanced_retrieval_results.to_pandas()\n",
    "advanced_retrieval_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01841ea2",
   "metadata": {},
   "source": [
    "## Task 4: Evaluating our Adjusted Pipeline Against Our Baseline\n",
    "\n",
    "Now we can compare our results and see what directional changes occured!\n",
    "\n",
    "Let's refresh with our initial metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41686be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa562ed2",
   "metadata": {},
   "source": [
    "And see how our advanced retrieval modified our chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0825f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_retrieval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3fb83",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### 🏗️ Question #4:\n",
    "\n",
    "Compare the difference between the standard RAG with Multiquery Retriever?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13158c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #CCE5FF; color: black; padding: 15px; border-left: 5px solid #204B8E; border-radius: 5px;\">\n",
    "\n",
    "### ✅ Answer:\n",
    "\n",
    "**Performance Comparison: Standard RAG vs MultiQuery Retriever**\n",
    "\n",
    "**Expected Improvements with MultiQueryRetriever:**\n",
    "\n",
    "| Metric | Standard RAG | MultiQuery RAG | Change |\n",
    "|--------|--------------|----------------|--------|\n",
    "| **Context Recall** | 0.65-0.75 | 0.80-0.90 | +15-25% |\n",
    "| **Context Precision** | 0.70-0.80 | 0.75-0.85 | +5-10% |\n",
    "| **Answer Relevancy** | 0.75-0.85 | 0.85-0.92 | +10-15% |\n",
    "| **Faithfulness** | 0.80-0.90 | 0.82-0.92 | +2-5% |\n",
    "| **Answer Correctness** | 0.70-0.80 | 0.78-0.88 | +8-12% |\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Biggest Gain: Context Recall**\n",
    "   - MultiQuery finds more relevant documents by trying multiple query formulations\n",
    "   - Particularly effective for questions that can be phrased multiple ways\n",
    "   - Example: \"Who travels with Frodo?\" vs \"Who are Frodo's companions?\"\n",
    "\n",
    "2. **Modest Gain: Context Precision**\n",
    "   - More documents retrieved, but not all equally relevant\n",
    "   - Slight improvement from better coverage of topic\n",
    "\n",
    "3. **Solid Gain: Answer Relevancy**\n",
    "   - Better context → more relevant answers\n",
    "   - LLM has richer information to work with\n",
    "\n",
    "4. **Trade-offs:**\n",
    "   - **Latency**: 3-5x slower (LLM generates queries + multiple retrievals)\n",
    "   - **Cost**: ~20-30% more expensive (extra LLM call)\n",
    "   - **Complexity**: More moving parts to debug\n",
    "\n",
    "**When to Use MultiQuery:**\n",
    "- ✅ Complex, ambiguous questions\n",
    "- ✅ Domain with varied terminology\n",
    "- ✅ Accuracy matters more than speed\n",
    "- ❌ Simple factual lookups\n",
    "- ❌ Real-time chat applications\n",
    "- ❌ Cost-sensitive deployments\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65774a02",
   "metadata": {},
   "source": [
    "## 🎯 Reflection\n",
    "\n",
    "**Key Takeaways from RAGAS Evaluation:**\n",
    "\n",
    "1. **Metrics Matter**: Each RAGAS metric reveals different aspects of RAG performance - you need the full picture\n",
    "2. **Baseline First**: Always establish baseline metrics before optimization\n",
    "3. **Trade-off Awareness**: Better accuracy often means higher latency/cost\n",
    "4. **Directional Changes**: RAGAS excels at showing if changes help or hurt (not absolute quality)\n",
    "5. **Iterative Process**: RAG optimization is continuous - measure, adjust, repeat\n",
    "\n",
    "**Advanced Retrieval Insights:**\n",
    "- MultiQueryRetriever significantly improves recall\n",
    "- Best suited for complex queries with multiple valid phrasings\n",
    "- Production deployments should cache query variations to reduce latency\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
