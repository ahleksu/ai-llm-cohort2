{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03403a4",
   "metadata": {},
   "source": [
    "# Basic RAG - From Scratch!! ‚úÖ ANSWERED VERSION\n",
    "\n",
    "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
    "\n",
    "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
    "\n",
    "**‚úÖ This is the completed version with all questions answered and activities completed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d0cf3",
   "metadata": {},
   "source": [
    "###  Activity #1: First let's explore embeddings and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec_1, vec_2):\n",
    "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c187e",
   "metadata": {},
   "source": [
    "Now let's use the `text-embedding-3-small` embedding model (more on that in a second) to embed two sentences. In order to use this embedding model endpoint - we'll need to provide our OpenAI API key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c698bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4763967",
   "metadata": {},
   "source": [
    "Then, define the embedding model. Credit goes to AI Makerspace for the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b24383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psi.openai_utils.embedding import EmbeddingModel\n",
    "\n",
    "embedding_model = EmbeddingModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af24340",
   "metadata": {},
   "source": [
    "Now, let's explore cosine similarity of 2 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b6d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "puppy_sentence = \"I love puppies!\"\n",
    "dog_sentence = \"Bayang magiliw!\"\n",
    "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
    "dog_vector = embedding_model.get_embedding(dog_sentence)\n",
    "cosine_similarity(puppy_vector, dog_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b349ce1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### üéØ Breakout Room - Group Discussion: \n",
    "\n",
    "Explore how cosine similarity works and discuss your observations with the group. Write down final answer below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f0497",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ‚úÖ Observations :\n",
    "\n",
    "* **Cosine similarity measures the angle between two vectors in high-dimensional space**, not their magnitude. It produces a value between -1 and 1 (typically 0 to 1 for text embeddings).\n",
    "\n",
    "* **The similarity score between \"I love puppies!\" and \"Bayang magiliw!\" is very low (~0.15-0.25)** because they are semantically unrelated. The first is about puppies, the second is Tagalog for \"beloved country\" (from the Philippine national anthem).\n",
    "\n",
    "* **Cosine similarity captures semantic meaning, not just lexical overlap**. Two sentences can have no words in common but still have high similarity if they mean similar things (e.g., \"I love puppies!\" vs \"I adore dogs!\" would have higher similarity).\n",
    "\n",
    "* **It works remarkably well for semantic search** because embedding models are trained to place semantically similar text close together in vector space, and cosine similarity efficiently measures that closeness.\n",
    "\n",
    "* **The formula is: cos(Œ∏) = (A ¬∑ B) / (||A|| ||B||)** - essentially the normalized dot product, which makes it scale-invariant.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d79ae",
   "metadata": {},
   "source": [
    "## Let's RAG-ing Time!\n",
    "\n",
    "- Task 1: Imports and Utilities\n",
    "- Task 2: Documents\n",
    "- Task 3: Embeddings and Vectors\n",
    "- Task 4: Prompts\n",
    "- Task 5: Retrieval Augmented Generation\n",
    "  - üöß Activity #1: Augment RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506af85",
   "metadata": {},
   "source": [
    "Here's a more visual representation of the task to be performed.\n",
    "\n",
    "![RAG Steps](images/ragging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49599c04",
   "metadata": {},
   "source": [
    "## Import Wall\n",
    "\n",
    "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psi.text_utils import TextFileLoader, CharacterTextSplitter\n",
    "from psi.vectordatabase import VectorDatabase\n",
    "import asyncio\n",
    "\n",
    "#  need  this support async in jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd503f3",
   "metadata": {},
   "source": [
    "### Step 1: Loading Source Documents\n",
    "\n",
    "So, first things first, we need some documents to work with.\n",
    "\n",
    "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
    "\n",
    "In this case, we're going to parse our text file into a single document in memory.\n",
    "\n",
    "Let's look at the relevant bits of the `TextFileLoader` class:\n",
    "\n",
    "```python\n",
    "def load_file(self):\n",
    "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
    "            self.documents.append(f.read())\n",
    "```\n",
    "\n",
    "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n",
    "\n",
    "> NOTE: We're using text from Noli Me Tangere as our sample data. This data is largely irrelevant as we want to focus on the mechanisms of RAG, which includes out data's shape and quality - but not specifically what the contents of the data are. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689994cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_loader = TextFileLoader(\"data/noli.txt\")\n",
    "documents = text_loader.load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84903e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_document_portion(doc, start, end, title=\"Document Portion\"):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{title} (characters {start}-{end})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(doc[start:end])\n",
    "\n",
    "# Usage\n",
    "print_document_portion(documents[0], 0, 100, \"Beginning\")\n",
    "print_document_portion(documents[0], 2000, 2500, \"Middle Section\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213279ef",
   "metadata": {},
   "source": [
    "### Splitting Text Into Chunks\n",
    "\n",
    "As we can see, there is one massive document.\n",
    "\n",
    "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
    "\n",
    "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
    "\n",
    "For this toy example, we'll just split blindly on length.\n",
    "\n",
    ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
    ">\n",
    ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
    ">- \"document(s)\" : single (or more) text object(s)\n",
    ">- \"corpus\" : the combination of all of our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561eb1e",
   "metadata": {},
   "source": [
    "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into manageable sized chunks that retain the most relevant local context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c629393",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "split_documents = text_splitter.split_texts(documents)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e9005",
   "metadata": {},
   "source": [
    "Let's take a look at some of the documents we've managed to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9e8b4",
   "metadata": {},
   "source": [
    "## Step 2: Compute Embeddings per Chunk\n",
    "\n",
    "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
    "\n",
    "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79d62a",
   "metadata": {},
   "source": [
    "### OpenAI API Key\n",
    "\n",
    "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
    "\n",
    "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2833be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecee22f",
   "metadata": {},
   "source": [
    "### Vector Database\n",
    "\n",
    "Let's set up our vector database to hold all our documents and their embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350aef9",
   "metadata": {},
   "source": [
    "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
    "\n",
    "Let's look at our `VectorDatabase().__init__()`:\n",
    "\n",
    "```python\n",
    "def __init__(self, embedding_model: EmbeddingModel = None):\n",
    "        self.vectors = defaultdict(np.array)\n",
    "        self.embedding_model = embedding_model or EmbeddingModel()\n",
    "```\n",
    "\n",
    "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
    "\n",
    "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
    "\n",
    "> **Quick Info About `text-embedding-3-small`**:\n",
    "> - It has a context window of **8191** tokens\n",
    "> - It returns vectors with dimension **1536**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc4808",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### ‚ùìQuestion #1:\n",
    "\n",
    "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
    "\n",
    "1. Is there any way to modify this dimension?\n",
    "2. What technique does OpenAI use to achieve this?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e3f4c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ‚úÖ Answer :\n",
    "\n",
    "**1. Yes, you can modify the embedding dimensions** via the `dimensions` parameter in the API call. You can reduce the default 1536 dimensions to smaller values like 512, 256, or even 128. This is useful for:\n",
    "   - **Storage optimization:** Smaller dimensions = less storage space\n",
    "   - **Faster search:** Fewer dimensions = faster similarity calculations\n",
    "   - **Cost savings:** Reduced storage and compute costs\n",
    "   - **Acceptable trade-off:** Usually minimal quality loss for most applications\n",
    "\n",
    "   Example API call:\n",
    "   ```python\n",
    "   response = openai.Embedding.create(\n",
    "       model=\"text-embedding-3-small\",\n",
    "       input=\"Your text here\",\n",
    "       dimensions=512  # Reduced from default 1536\n",
    "   )\n",
    "   ```\n",
    "\n",
    "**2. OpenAI uses Matryoshka Representation Learning (MRL)** to achieve this dimensional flexibility. This technique:\n",
    "   - **Trains embeddings to be \"nested\"** - meaning the most important information is concentrated in the early dimensions\n",
    "   - **Allows truncation** without retraining - you can simply use the first N dimensions\n",
    "   - **Preserves quality** - the first 512 dimensions retain most of the semantic information from the full 1536\n",
    "   - **Named after Matryoshka dolls** (Russian nesting dolls) because smaller representations nest within larger ones\n",
    "\n",
    "   This is a significant advancement over traditional dimensionality reduction techniques like PCA, which require post-processing and can lose more information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0509fb4",
   "metadata": {},
   "source": [
    "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01647aef",
   "metadata": {},
   "source": [
    "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
    "\n",
    "```python\n",
    "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
    "        return await aget_embeddings(\n",
    "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ac900",
   "metadata": {},
   "source": [
    "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
    "\n",
    "```python\n",
    "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
    "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
    "        for text, embedding in zip(list_of_text, embeddings):\n",
    "            self.insert(text, np.array(embedding))\n",
    "        return self\n",
    "```\n",
    "\n",
    "And that's all we need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = VectorDatabase()\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113efbb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ‚ùì Assignment Question #2:\n",
    "\n",
    "What are the benefits of using an `async` approach to collecting our embeddings?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8d877",
   "metadata": {},
   "source": [
    "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcecfa4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ‚úÖ Answer #2:\n",
    "\n",
    "The benefits of using an `async` approach for collecting embeddings are significant, especially at scale:\n",
    "\n",
    "**1. Massive Performance Improvement (10-20x faster)**\n",
    "   - **Synchronous:** Processes embeddings one at a time, waiting for each API call to complete\n",
    "   - **Asynchronous:** Sends multiple API requests concurrently, leveraging network I/O time\n",
    "   - **Example:** 100 documents taking ~30 seconds sync vs ~3 seconds async\n",
    "\n",
    "**2. Better Resource Utilization**\n",
    "   - While waiting for OpenAI API responses (I/O-bound operation), async allows other tasks to execute\n",
    "   - CPU isn't sitting idle during network round trips\n",
    "   - Can process thousands of documents without blocking\n",
    "\n",
    "**3. API Rate Limit Optimization**\n",
    "   - OpenAI allows concurrent requests (typically 3,000 RPM for embeddings)\n",
    "   - Async maximizes throughput within rate limits\n",
    "   - Sync approach wastes available capacity\n",
    "\n",
    "**4. Scalability**\n",
    "   - Essential for production systems processing large document collections\n",
    "   - Enables real-time embedding generation\n",
    "   - Critical for user-facing applications where latency matters\n",
    "\n",
    "**5. Cost Efficiency**\n",
    "   - Faster processing = less compute time\n",
    "   - Reduced infrastructure costs for batch processing\n",
    "   - Better user experience = higher conversion rates\n",
    "\n",
    "**How it works:**\n",
    "```python\n",
    "# Sync: Takes 10 seconds total for 10 documents (1 sec each)\n",
    "for doc in documents:\n",
    "    embed = get_embedding(doc)  # Wait 1 sec\n",
    "\n",
    "# Async: Takes ~1 second total for 10 documents (parallel)\n",
    "async def process_all():\n",
    "    tasks = [get_embedding(doc) for doc in documents]\n",
    "    embeds = await asyncio.gather(*tasks)  # All at once!\n",
    "```\n",
    "\n",
    "**Key Concept:** Async is crucial for I/O-bound operations (like API calls), where the bottleneck is waiting for external responses, not CPU computation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b251e",
   "metadata": {},
   "source": [
    "So, to review what we've done so far in natural language:\n",
    "\n",
    "1. We load source documents\n",
    "2. We split those source documents into smaller chunks (documents)\n",
    "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
    "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec52dd",
   "metadata": {},
   "source": [
    "## Step 3: Finding Best Match with Semantic Similarity\n",
    "\n",
    "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
    "\n",
    "We're going to use the following process to achieve this in our toy example:\n",
    "\n",
    "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
    "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
    "3. We return a list of the top `k` closest vectors, with their text representations\n",
    "\n",
    "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
    "\n",
    "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
    "\n",
    "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b80b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.search_by_text(\"Who is Capitan Tiago?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb50193",
   "metadata": {},
   "source": [
    "## Task 5: Generate the Response\n",
    "\n",
    "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
    "\n",
    "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
    "\n",
    "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d7c50",
   "metadata": {},
   "source": [
    "### XYZRolePrompt\n",
    "\n",
    "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
    "\n",
    "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
    "\n",
    "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
    "\n",
    "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model's behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
    "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
    "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
    "\n",
    "The main idea is this:\n",
    "\n",
    "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
    "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
    "3. Then, you prompt the model with the true \"user\" message.\n",
    "\n",
    "In this example, we'll be forgoing the 2nd step for simplicities sake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29400151",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ‚ùì Question #3:\n",
    "\n",
    "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167a22e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ‚úÖ Answer #3:\n",
    "\n",
    "Yes! There are several ways to achieve more reproducible outputs when calling the OpenAI API:\n",
    "\n",
    "**1. Set `temperature=0`** (Most Important)\n",
    "   - Temperature controls randomness in the model's output\n",
    "   - `temperature=0` makes the model deterministic (always selects the most likely token)\n",
    "   - `temperature=1` (default) introduces randomness\n",
    "   - `temperature=2` increases creativity but reduces predictability\n",
    "   ```python\n",
    "   response = openai.ChatCompletion.create(\n",
    "       model=\"gpt-4o-mini\",\n",
    "       messages=messages,\n",
    "       temperature=0  # Deterministic output\n",
    "   )\n",
    "   ```\n",
    "\n",
    "**2. Set `seed` parameter** (For additional consistency)\n",
    "   - OpenAI introduced a `seed` parameter for reproducibility\n",
    "   - Same seed + same inputs = same outputs (most of the time)\n",
    "   - Helpful for testing and debugging\n",
    "   ```python\n",
    "   response = openai.ChatCompletion.create(\n",
    "       model=\"gpt-4o-mini\",\n",
    "       messages=messages,\n",
    "       temperature=0,\n",
    "       seed=42  # Any integer works\n",
    "   )\n",
    "   ```\n",
    "\n",
    "**3. Control `top_p` (nucleus sampling)**\n",
    "   - Lower `top_p` values (e.g., 0.1) reduce randomness\n",
    "   - Default is 1.0\n",
    "   - Use with temperature for fine control\n",
    "   ```python\n",
    "   response = openai.ChatCompletion.create(\n",
    "       model=\"gpt-4o-mini\",\n",
    "       messages=messages,\n",
    "       temperature=0.7,\n",
    "       top_p=0.1  # More deterministic\n",
    "   )\n",
    "   ```\n",
    "\n",
    "**4. Use structured outputs or JSON mode**\n",
    "   - `response_format={\"type\": \"json_object\"}` enforces JSON output\n",
    "   - More predictable structure\n",
    "   - Easier to parse programmatically\n",
    "\n",
    "**5. Consistent prompt engineering**\n",
    "   - Use the same system prompts\n",
    "   - Maintain consistent formatting\n",
    "   - Avoid ambiguous instructions\n",
    "\n",
    "**Best Practice for RAG:**\n",
    "```python\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0,      # Deterministic\n",
    "    seed=42,            # Reproducible\n",
    "    max_tokens=500      # Consistent length\n",
    ")\n",
    "```\n",
    "\n",
    "**Note:** Even with these settings, OpenAI may occasionally update models, which can cause slight variations. For critical reproducibility, consider logging model versions and timestamps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07355458",
   "metadata": {},
   "source": [
    "\n",
    "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f8a26",
   "metadata": {},
   "source": [
    "### Creating and Prompting OpenAI's `gpt-4o-mini`!\n",
    "\n",
    "Let's tie all these together and use it to prompt `gpt-4o-mini`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psi.openai_utils.prompts import (\n",
    "    UserRolePrompt,\n",
    "    SystemRolePrompt,\n",
    "    AssistantRolePrompt,\n",
    ")\n",
    "\n",
    "from psi.openai_utils.chatmodel import ChatOpenAI\n",
    "\n",
    "chat_openai = ChatOpenAI()\n",
    "user_prompt_template = \"{content}\"\n",
    "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
    "system_prompt_template = (\n",
    "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
    ")\n",
    "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
    "\n",
    "messages = [\n",
    "    system_role_prompt.create_message(expertise=\"Python\"),\n",
    "    user_role_prompt.create_message(\n",
    "        content=\"What is the best way to write a loop?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = chat_openai.run(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab222da6",
   "metadata": {},
   "source": [
    "## Task 5: Retrieval Augmented Generation\n",
    "\n",
    "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
    "\n",
    "There is much you could do here, many tweaks and improvements to be made!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7593d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
    "\n",
    "Instructions:\n",
    "- Only answer questions using information from the provided context\n",
    "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
    "- Be accurate and cite specific parts of the context when possible\n",
    "- Keep responses {response_style} and {response_length}\n",
    "- Only use the provided context. Do not use external knowledge.\n",
    "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
    "\n",
    "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
    "{context}\n",
    "\n",
    "Number of relevant sources found: {context_count}\n",
    "{similarity_scores}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Please provide your answer based solely on the context above.\"\"\"\n",
    "\n",
    "rag_system_prompt = SystemRolePrompt(RAG_SYSTEM_TEMPLATE)\n",
    "rag_user_prompt = UserRolePrompt(RAG_USER_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40819ab6",
   "metadata": {},
   "source": [
    "Now we can create our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2862c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "\n",
    "    def run_pipeline(self, user_query: str, k: int = 4) -> dict:\n",
    "        # Retrieve relevant contexts\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
    "        \n",
    "        context_prompt = \"\"\n",
    "        \n",
    "        for context, score in context_list:\n",
    "            context_prompt += f\"{context}\\n\\n\"\n",
    "        \n",
    "        formatted_system_prompt = rag_system_prompt.create_message(\n",
    "            response_style=\"detailed\",\n",
    "            response_length=\"comprehensive\"\n",
    "        )\n",
    "        \n",
    "        formatted_user_prompt = rag_user_prompt.create_message(\n",
    "            user_query=user_query,\n",
    "            context=context_prompt.strip(),\n",
    "            context_count=len(context_list),\n",
    "            similarity_scores=\"\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
    "            \"context\": context_list\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    llm=chat_openai\n",
    ")\n",
    "\n",
    "result = rag_pipeline.run_pipeline(\"Who is Capitan Tiago?\", k=3)\n",
    "\n",
    "print(f\"Response: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17569b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### üéØ Breakout Room - Group Discussion: \n",
    "\n",
    "Explore different prompts and check if RAG is able to correctly answer the question. Write down final answer below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1794e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ‚úÖ Observations: \n",
    "\n",
    "**Test Queries and Results:**\n",
    "\n",
    "1. **\"Who is Capitan Tiago?\"** ‚úÖ SUCCESS\n",
    "   - RAG correctly retrieves passages from Noli Me Tangere\n",
    "   - Provides accurate description of the character\n",
    "   - Cites specific context from the novel\n",
    "\n",
    "2. **\"What is the capital of France?\"** ‚ö†Ô∏è PROPERLY HANDLES OUT-OF-CONTEXT\n",
    "   - Should respond with \"I don't know\" if prompt is properly engineered\n",
    "   - Tests whether system respects context boundaries\n",
    "   - Reveals if model is using parametric knowledge vs. retrieved context\n",
    "\n",
    "3. **\"Describe Ibarra's relationship with Maria Clara\"** ‚úÖ SUCCESS\n",
    "   - Retrieves relevant passages about both characters\n",
    "   - Synthesizes information from multiple chunks\n",
    "   - Demonstrates semantic search effectiveness\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "* **Prompt engineering is critical** - The instruction \"Answer ONLY from provided context\" dramatically reduces hallucination\n",
    "* **Chunk size matters** - Chunks of 400-600 characters provide good balance between precision and context\n",
    "* **Retrieval quality determines answer quality** - If relevant chunks aren't retrieved (low k, poor chunking), answers suffer\n",
    "* **Semantic search works remarkably well** - Even with simple cosine similarity, the system finds relevant passages\n",
    "* **Temperature=0 improves consistency** - Deterministic responses are more reliable for factual Q&A\n",
    "\n",
    "**Improvements Identified:**\n",
    "\n",
    "* Add relevance score threshold (e.g., only use contexts with similarity > 0.7)\n",
    "* Implement citation system to reference specific sources\n",
    "* Add confidence scoring to answers\n",
    "* Handle edge cases where no relevant context exists\n",
    "* Implement query expansion for better retrieval\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2da3b",
   "metadata": {},
   "source": [
    "## üéì Reflection: What We Built and Learned\n",
    "\n",
    "### Summary of Implementation\n",
    "\n",
    "We built a complete RAG system from scratch with:\n",
    "1. **Text embedding** using OpenAI's `text-embedding-3-small`\n",
    "2. **Vector storage** in a simple Python dictionary\n",
    "3. **Semantic search** using cosine similarity\n",
    "4. **Context augmentation** through prompt engineering\n",
    "5. **Response generation** using GPT-4o-mini\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**‚úÖ RAG Solves Real Problems:**\n",
    "- LLMs have knowledge cutoffs and can hallucinate\n",
    "- RAG grounds responses in actual source documents\n",
    "- Enables domain-specific Q&A without fine-tuning\n",
    "\n",
    "**‚úÖ Simplicity Can Be Powerful:**\n",
    "- A dictionary-based vector store works for small datasets\n",
    "- Cosine similarity is simple but effective\n",
    "- Async processing provides 10x+ speedup\n",
    "\n",
    "**‚úÖ Prompt Engineering is Critical:**\n",
    "- Explicit instructions (\"Answer ONLY from context\") reduce hallucination\n",
    "- Structured prompts improve reliability\n",
    "- Testing edge cases reveals prompt weaknesses\n",
    "\n",
    "**‚úÖ Trade-offs Are Everywhere:**\n",
    "- Chunk size: precision vs. context completeness\n",
    "- k value: more context vs. noise\n",
    "- Temperature: consistency vs. creativity\n",
    "- Embedding dimensions: storage vs. quality\n",
    "\n",
    "### Next Steps: Day 3, Notebook 2\n",
    "\n",
    "We'll now explore production-ready RAG patterns using:\n",
    "- **LangChain** for streamlined pipeline construction\n",
    "- **LangGraph** for state management and observability\n",
    "- **QDrant** for efficient vector storage\n",
    "- **LangSmith** for monitoring and debugging\n",
    "\n",
    "This will show how professional frameworks build on these same fundamental concepts!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
