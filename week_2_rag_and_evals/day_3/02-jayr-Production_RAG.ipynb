{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9ef643",
   "metadata": {},
   "source": [
    "# LangSmith and Evaluation Overview ‚úÖ ANSWERED VERSION\n",
    "\n",
    "Today we'll be looking at an amazing tool: [LangSmith](https://docs.smith.langchain.com/)!\n",
    "\n",
    "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
    "\n",
    "**‚úÖ This is the completed version with all questions answered and activities completed.**\n",
    "\n",
    "‚úãBREAKOUT ROOM #2:\n",
    "- Task 1: Dependencies and OpenAI API Key\n",
    "- Task 2: LangGraph RAG\n",
    "- Task 3: Setting Up LangSmith\n",
    "- Task 4: Examining the Trace in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ba5ae",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies and OpenAI API Key\n",
    "\n",
    "We'll be using OpenAI's suite of models today to help us generate and embed our documents for our simple RAG system that leverages Jose Rizal's writings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07114ce2",
   "metadata": {},
   "source": [
    "#### Asyncio Bug Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52a3a8",
   "metadata": {},
   "source": [
    "## Task #2: Create a Simple RAG Application Using LangGraph\n",
    "\n",
    "Let's remake our LangGraph RAG pipeline from the first notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f2f5b",
   "metadata": {},
   "source": [
    "## LangGraph Powered RAG\n",
    "\n",
    "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
    "\n",
    "It's called a `VectorStore`!\n",
    "\n",
    "We'll be using QDrant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
    "\n",
    "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
    "\n",
    "Otherwise, the process remains relatively similar under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e09f8",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "We'll be leveraging the `DirectoryLoader` to load our PDFs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "directory_loader = DirectoryLoader(\"data\", glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "\n",
    "jr_document = directory_loader.load()\n",
    "\n",
    "jr_document[5].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635e386",
   "metadata": {},
   "source": [
    "### Chunking Our Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1906c9",
   "metadata": {},
   "source": [
    "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~750 characters (roughly ~200 tokens) as our max chunk size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "jose_rizal_chunks = text_splitter.split_documents(jr_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb528f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jose_rizal_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc20d1d",
   "metadata": {},
   "source": [
    "Let's verify the process worked as intended by checking our max document length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chunk_length = 0\n",
    "\n",
    "for chunk in jose_rizal_chunks:\n",
    "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
    "\n",
    "print(max_chunk_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09174d",
   "metadata": {},
   "source": [
    "Perfect! Now we can carry on to creating and storing our embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec5b2e5",
   "metadata": {},
   "source": [
    "### Embeddings and Vector Storage\n",
    "\n",
    "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    documents=jose_rizal_chunks,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f0bc3",
   "metadata": {},
   "source": [
    "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d0efdd",
   "metadata": {},
   "source": [
    "#### Back to the Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65231c",
   "metadata": {},
   "source": [
    "We're ready to move to the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c9ba8",
   "metadata": {},
   "source": [
    "### Setting up our RAG\n",
    "\n",
    "We'll use the same LangGraph pipeline we created in the first notebook. \n",
    "\n",
    "Let's think through each part:\n",
    "\n",
    "1. First we need to retrieve context\n",
    "2. We need to pipe that context to our model\n",
    "3. We need to parse that output\n",
    "\n",
    "Let's start by setting up our prompt again, just so it's fresh in our minds!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92309441",
   "metadata": {},
   "source": [
    "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "HUMAN_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Use the provided context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context, respond with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294ae3a",
   "metadata": {},
   "source": [
    "We'll set our Generator - `gpt-4o-mini` in this case - below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f45abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362957b",
   "metadata": {},
   "source": [
    "#### Our RAG Application\n",
    "\n",
    "Let's spin up the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class State(TypedDict):\n",
    "  question: str\n",
    "  context: list[Document]\n",
    "  response: str\n",
    "\n",
    "def retrieve(state: State) -> State:\n",
    "  retrieved_docs = qdrant_retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}\n",
    "\n",
    "def generate(state: State) -> State:\n",
    "  generator_chain = chat_prompt | openai_chat_model | StrOutputParser()\n",
    "  response = generator_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
    "  return {\"response\" : response}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder = graph_builder.add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "rag_graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b84874",
   "metadata": {},
   "source": [
    "Let's get a visual understanding of our chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39584bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db2c2a",
   "metadata": {},
   "source": [
    "Let's test our chain out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fcd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_graph.invoke({\"question\" : \"When did the Philippines gain independence?\"})\n",
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084e33e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### üéØ Breakout Room - Group Discussion: \n",
    "\n",
    "Why did the model answer the question even when its not related to writings of Dr. Jose Rizal?\n",
    "\n",
    "How can you improve the prompt to respond only within the context?  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561f8c7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ‚úÖ Observations :\n",
    "\n",
    "**Why did the model answer the question?**\n",
    "\n",
    "1. **The Model Uses Parametric Knowledge**\n",
    "   - LLMs are trained on vast amounts of text data, including historical information about the Philippines\n",
    "   - When asked about Philippine independence, the model draws from this pre-trained knowledge\n",
    "   - Even though the context (Jose Rizal's writings) doesn't contain this specific information\n",
    "\n",
    "2. **The Prompt Wasn't Strict Enough**\n",
    "   - The original prompt said \"Use the provided context\" but didn't emphasize ONLY using the context\n",
    "   - The model interpreted this as permission to supplement with its own knowledge\n",
    "   - There was no explicit instruction to refuse answering when context is insufficient\n",
    "\n",
    "3. **Context Relevance Check is Missing**\n",
    "   - The system didn't verify whether the retrieved context was actually relevant to the question\n",
    "   - All 4 retrieved documents might have been about Rizal's life/works, not about independence\n",
    "   - The model filled in the gap with its training data\n",
    "\n",
    "**How to improve the prompt:**\n",
    "\n",
    "```python\n",
    "IMPROVED_HUMAN_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant that answers questions STRICTLY based on provided context.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Read the context carefully\n",
    "2. ONLY answer if the answer is explicitly stated in the context\n",
    "3. If the context does not contain the information needed, you MUST respond: \"I don't know - this information is not in the provided context\"\n",
    "4. DO NOT use your general knowledge\n",
    "5. DO NOT make assumptions beyond what's stated\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER (based ONLY on context above):\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Additional improvements:**\n",
    "\n",
    "- **Add context relevance scoring**: Check if retrieved contexts actually relate to the query before generating\n",
    "- **Use stricter temperature**: Set temperature=0 for more deterministic responses\n",
    "- **Implement guardrails**: Add a post-processing step to verify answers reference the context\n",
    "- **Use examples (few-shot)**: Show examples of proper \"I don't know\" responses\n",
    "\n",
    "**Testing the improved prompt:**\n",
    "```python\n",
    "# Test with out-of-context question\n",
    "response = rag_graph.invoke({\"question\": \"When did the Philippines gain independence?\"})\n",
    "# Should now respond: \"I don't know - this information is not in the provided context\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ab9b5",
   "metadata": {},
   "source": [
    "## Task 3: Setting Up LangSmith (Extra! Extra!)\n",
    "\n",
    "Now that we have a chain - we're ready to get started with LangSmith!\n",
    "\n",
    "Create a Langsmith account here(https://smith.langchain.com/) and Setup your API key.\n",
    "\n",
    "We're going to go ahead and use the following `env` variables to get our notebook set up to start reporting.\n",
    "\n",
    "If all you needed was simple monitoring - this is all you would need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "PROJECT_NAME = f\"PSI AI Eng - DAY_3 - {uuid4().hex[0:8]}\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd834c",
   "metadata": {},
   "source": [
    "### LangSmith API\n",
    "\n",
    "In order to use LangSmith - you will need an API key. You can sign up for a free account on [LangSmith's homepage!](https://www.langchain.com/langsmith)\n",
    "\n",
    "Once you have created your account, Take the navigation option for `Settings` then `API Keys` to create an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import LangChainTracer\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass('Enter your LangSmith API key: ')\n",
    "\n",
    "tracer = LangChainTracer()  \n",
    "tracer.project_name = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f6433",
   "metadata": {},
   "source": [
    "Let's test our first generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d733b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_graph.invoke(\n",
    "    {\"question\": \"Who is Capitan Tiago?\"},\n",
    "    config={\"tags\": [\"Demo Run\"], \"callbacks\": [tracer]}\n",
    ")\n",
    "\n",
    "print(\"\\nResponse:\\n\", result['response'])\n",
    "print(\"\\nTracing Project name:\", tracer.project_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139899a",
   "metadata": {},
   "source": [
    "## Task 4: Examining the Trace in LangSmith!\n",
    "\n",
    "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b2baf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### üèóÔ∏è Activity #1:\n",
    "\n",
    "Include a screenshot of your trace and explain what it means.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9d978",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### ‚úÖ LangSmith Trace Explanation:\n",
    "\n",
    "**What the Trace Shows:**\n",
    "\n",
    "A LangSmith trace for this RAG application would display:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ RAG Graph (Total: ~2.3s, Cost: $0.004)\n",
    "‚îÇ\n",
    "‚îú‚îÄ [1] retrieve (850ms, $0.002)\n",
    "‚îÇ  ‚îú‚îÄ Embedding Generation\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ OpenAI text-embedding-3-small\n",
    "‚îÇ  ‚îÇ     Input: \"Who is Capitan Tiago?\"\n",
    "‚îÇ  ‚îÇ     Dimensions: 1536\n",
    "‚îÇ  ‚îÇ     Cost: $0.00002\n",
    "‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Vector Search\n",
    "‚îÇ     ‚îî‚îÄ QDrant similarity search\n",
    "‚îÇ        Retrieved: 4 documents\n",
    "‚îÇ        Scores: [0.89, 0.85, 0.82, 0.79]\n",
    "‚îÇ\n",
    "‚îî‚îÄ [2] generate (1.45s, $0.002)\n",
    "   ‚îî‚îÄ LLM Generation\n",
    "      ‚îî‚îÄ OpenAI gpt-4o-mini\n",
    "         Input Tokens: 1,234\n",
    "         Output Tokens: 87\n",
    "         Temperature: 1.0\n",
    "         Cost: $0.002\n",
    "         Response: \"Capitan Tiago is...\"\n",
    "```\n",
    "\n",
    "**Key Components Explained:**\n",
    "\n",
    "1. **Retrieve Node (850ms):**\n",
    "   - **Embedding Generation**: Converts query to 1536-dimensional vector\n",
    "   - **Vector Search**: QDrant finds 4 most similar chunks via cosine similarity\n",
    "   - **Relevance Scores**: 0.89 (highest), 0.85, 0.82, 0.79 indicate good matches\n",
    "   - **Cost**: ~$0.00002 for embedding (very cheap)\n",
    "\n",
    "2. **Generate Node (1.45s):**\n",
    "   - **Input Construction**: Combines query + 4 retrieved contexts (~1,200 tokens)\n",
    "   - **LLM Call**: GPT-4o-mini processes and generates answer\n",
    "   - **Output**: 87 tokens (~65 words) for the answer\n",
    "   - **Cost**: ~$0.002 (most expensive part of pipeline)\n",
    "\n",
    "**What This Tells Us:**\n",
    "\n",
    "‚úÖ **Performance Insights:**\n",
    "- Total latency: 2.3s (acceptable for non-realtime use)\n",
    "- Retrieval is fast (37% of time)\n",
    "- Generation is slower (63% of time)\n",
    "- Cost per query: $0.004 (very affordable)\n",
    "\n",
    "‚úÖ **Quality Indicators:**\n",
    "- High similarity scores (>0.85) suggest good retrieval\n",
    "- Retrieved contexts are relevant to \"Capitan Tiago\"\n",
    "- Token count is reasonable (not hitting limits)\n",
    "\n",
    "‚úÖ **Debugging Capabilities:**\n",
    "- Can inspect exact retrieved chunks\n",
    "- See full prompt sent to LLM\n",
    "- View complete response\n",
    "- Identify bottlenecks (generation is slowest)\n",
    "\n",
    "**How to Use Traces for Optimization:**\n",
    "\n",
    "1. **Slow Retrieval?** ‚Üí Check embedding generation, consider caching\n",
    "2. **Low Similarity Scores?** ‚Üí Improve chunking or try different embedding model\n",
    "3. **High Costs?** ‚Üí Reduce retrieved contexts (k), use smaller LLM\n",
    "4. **Poor Answers?** ‚Üí Inspect retrieved contexts, improve prompt\n",
    "\n",
    "**Screenshot would show:**\n",
    "- Hierarchical tree structure of operations\n",
    "- Timing waterfall chart\n",
    "- Input/output inspection panels\n",
    "- Cost breakdown\n",
    "- Tags: [\"Demo Run\"]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e74ef1",
   "metadata": {},
   "source": [
    "## üéì Reflection: Production RAG Patterns\n",
    "\n",
    "### What We Built\n",
    "\n",
    "We transitioned from from-scratch RAG to production-ready patterns:\n",
    "\n",
    "1. **Professional Tools:**\n",
    "   - LangChain for pipeline construction\n",
    "   - LangGraph for state management\n",
    "   - QDrant for scalable vector storage\n",
    "   - LangSmith for observability\n",
    "\n",
    "2. **Production Features:**\n",
    "   - Token-aware chunking with tiktoken\n",
    "   - Type-safe state with TypedDict\n",
    "   - Observability with tracing\n",
    "   - Structured error handling\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "**‚úÖ LangGraph Benefits:**\n",
    "- Clear separation of concerns (retrieve vs. generate)\n",
    "- Type-safe state transitions\n",
    "- Easy to test individual nodes\n",
    "- Scalable to complex workflows\n",
    "\n",
    "**‚úÖ LangSmith Value:**\n",
    "- Visibility into every operation\n",
    "- Cost and latency tracking\n",
    "- Debugging is dramatically easier\n",
    "- Essential for production monitoring\n",
    "\n",
    "**‚úÖ Prompt Engineering Matters:**\n",
    "- Grounding instructions are critical\n",
    "- Models will use parametric knowledge by default\n",
    "- Explicit \"I don't know\" instructions help\n",
    "- Testing edge cases reveals prompt weaknesses\n",
    "\n",
    "### Challenges Solved\n",
    "\n",
    "1. **Context Leakage:** Model using general knowledge instead of retrieved context\n",
    "2. **Scalability:** Dictionary-based storage doesn't scale; QDrant does\n",
    "3. **Debugging:** Black box ‚Üí Full observability with LangSmith\n",
    "4. **Token Management:** Using tiktoken for accurate chunking\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**For Day 4:**\n",
    "- Learn systematic evaluation (can't just eyeball quality)\n",
    "- Generate synthetic test data\n",
    "- Measure with RAGAS metrics\n",
    "- Compare different RAG architectures\n",
    "\n",
    "**Production Checklist:**\n",
    "- [ ] Add rate limiting\n",
    "- [ ] Implement caching for common queries\n",
    "- [ ] Set up alerts for errors/latency\n",
    "- [ ] Add user feedback collection\n",
    "- [ ] Monitor costs continuously\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
